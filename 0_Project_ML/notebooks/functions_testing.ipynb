{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "import os, sys\n",
    "\n",
    "# Relative paths\n",
    "dirname = os.path.dirname\n",
    "sep = os.sep\n",
    "\n",
    "ml_folder = dirname(os.getcwd())\n",
    "sys.path.append(ml_folder)\n",
    "\n",
    "from src.utils import mining_data_tb as md\n",
    "from src.utils import visualization_tb as vi\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dfs = md.read_all_data(2,[\"1_demographics\", \"2_dietary\", \"3_examination\", \"4_laboratory\", \"5_questionnaire\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_dfs(data_dfs):\n",
    "    files = {}\n",
    "    count = 0\n",
    "\n",
    "    for key, dfs in data_dfs.items():\n",
    "        key_ = key[:-2]\n",
    "\n",
    "        if count == 0:\n",
    "            files[key_] = dfs\n",
    "        else:\n",
    "            if key_ not in files.keys():\n",
    "                files[key_] = dfs\n",
    "            else:\n",
    "                files[key_] = pd.concat([files[key_], dfs])\n",
    "\n",
    "        count +=1\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_all_dfs(data_dfs_list):\n",
    "    #end_dfs = []\n",
    "    end_dfs = {}\n",
    "    \n",
    "    for data_dfs in data_dfs_list:\n",
    "        files = concatenate_dfs(data_dfs)\n",
    "        end_dfs = {**end_dfs, **files}\n",
    "        #end_dfs.append(files)\n",
    "\n",
    "    return end_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(end_dfs):\n",
    "    keys = list(end_dfs.keys())\n",
    "    f_df = end_dfs.pop(keys[0])\n",
    "\n",
    "    for name, df in end_dfs.items():\n",
    "        f_df = pd.merge(f_df, df, how = \"outer\", on = \"SEQN\")\n",
    "\n",
    "    return f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = concatenate_all_dfs(all_data_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = merge_dfs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WTDRD1_x, WTDR2D_x, DRABF_x, DRDINT_x, WTSAF2YR_x, LBXHCT_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_columns(df):\n",
    "    df = df.drop([\"WTDRD1_y\", \"WTDR2D_y\", \"DRABF_y\", \"DRDINT_y\", \"WTSAF2YR_y\", \"LBXHCT_y\"], axis = 1)\n",
    "\n",
    "    columns_correction = {\n",
    "        \"WTDRD1_x\" : \"WTDRD1\",\n",
    "        \"WTDR2D_x\" : \"WTDR2D\",\n",
    "        \"DRABF_x\" : \"DRABF\",\n",
    "        \"DRDINT_x\" : \"DRDINT\",\n",
    "        \"WTSAF2YR_x\" : \"WTSAF2YR\",\n",
    "        \"LBXHCT_x\" : \"LBXHCT\"\n",
    "    }\n",
    "\n",
    "    df = df.rename(columns = columns_correction)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = clean_columns(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(29400, 956)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heart_disease(df):\n",
    "    # Conditions to remove values of no interest from the columns of interest\n",
    "    cond_b = df.MCQ160B != 9\n",
    "    cond_c = df.MCQ160C != 7\n",
    "    cond_d = (df.MCQ160D != 9) & (df.MCQ160D != 7)\n",
    "    cond_e = df.MCQ160E != 9\n",
    "    cond_f = df.MCQ160F != 9\n",
    "\n",
    "    # Filter the data with the previous conditions\n",
    "    heart_df = df[(cond_b) & (cond_c) & (cond_d) & (cond_e) & (cond_f)]\n",
    "\n",
    "    # New column to group all heart diseases\n",
    "    heart_df[\"heart_disease\"] = 0\n",
    "\n",
    "    # Conditions to filter by any heart disease\n",
    "    pos_cond_b = heart_df.MCQ160B == 1\n",
    "    pos_cond_c = heart_df.MCQ160C == 1\n",
    "    pos_cond_d = heart_df.MCQ160D == 1\n",
    "    pos_cond_e = heart_df.MCQ160E == 1\n",
    "    pos_cond_f = heart_df.MCQ160F == 1\n",
    "\n",
    "    # Given the previous conditions, place a \"1\" in the column if they are matched\n",
    "    heart_df.loc[(pos_cond_b) | (pos_cond_c) | (pos_cond_d) | (pos_cond_e) | (pos_cond_f), \"heart_disease\"] = 1\n",
    "\n",
    "    return heart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test4 = heart_disease(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    27422\n",
       "1     1863\n",
       "Name: heart_disease, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "test4.heart_disease.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data(df, split, cv, epochs = 1, scaler = False, balance = None, seed = 42):\n",
    "    features_nom = list(df.columns)\n",
    "    features = [md.var_descr_detector(nom, variable_names) for nom in features_nom]  \n",
    "\n",
    "    ### Independent variables\n",
    "    X = np.array(df.iloc[:, 1:])\n",
    "\n",
    "    ### Dependent variable\n",
    "    y = np.array(df.iloc[:, 0])\n",
    "\n",
    "    ### Data scaling\n",
    "    if scaler:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "    ### Train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split, random_state = seed)\n",
    "\n",
    "    ### Balancing data\n",
    "    if balance != None:\n",
    "        sm = SMOTE(sampling_strategy = balance, random_state = seed, n_jobs = -1)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    ### Cross validation\n",
    "    kfold = RepeatedStratifiedKFold(n_splits = cv, n_repeats = epochs, random_state = seed)\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, features, kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test5 = test4.loc[:, [\"MCQ010\", \"RIAGENDR\", \"RIDAGEYR\", \"BPXDI1\", \"BPXSY1\", \"BMXWT\", \"DXDTOPF\", \"BMXWAIST\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'src.utils.mining_data_tb' has no attribute 'var_descr_detector'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-35f15374c193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-c8362d115418>\u001b[0m in \u001b[0;36mmodel_data\u001b[0;34m(df, split, cv, epochs, scaler, balance, seed)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeatures_nom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_descr_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_nom\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m### Independent variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c8362d115418>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeatures_nom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_descr_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_nom\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m### Independent variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'src.utils.mining_data_tb' has no attribute 'var_descr_detector'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, features, kfold = model_data(test5, split = .2, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_trainer(X_train, y_train, kfold, model, features = None):\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    count = 1\n",
    "\n",
    "    for (train, val) in kfold.split(X_train, y_train):\n",
    "        # Train-Validation sets\n",
    "        x_t, y_t = X_train[train], y_train[train]\n",
    "        x_v, y_v = X_train[val], y_train[val]\n",
    "\n",
    "\n",
    "        # Internal structure\n",
    "        y_t_unique, y_t_counts = np.unique(y_t, return_counts=True)\n",
    "        y_v_unique, y_v_counts = np.unique(y_v, return_counts=True)\n",
    "\n",
    "        # Training\n",
    "        model.fit(x_t, y_t)\n",
    "\n",
    "        # Scores\n",
    "        train_score = model.score(x_t, y_t)\n",
    "        val_score = model.score(x_v, y_v)\n",
    "\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "\n",
    "        print(f\"\\n-- Model {count} --\")\n",
    "        print(\"-\" * 25)\n",
    "        print(\"Set structure:\")\n",
    "        print(\"Train structure:\", dict(zip(y_t_unique, y_t_counts / len(y_t))))\n",
    "        print(\"Validation structure:\", dict(zip(y_v_unique, y_v_counts / len(y_v))))\n",
    "        print(\"-\" * 25)\n",
    "        print(\">train score:\", train_score)\n",
    "        print(\">test score:\", val_score)\n",
    "        print(\"#\" * 75)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    try:\n",
    "        importances = model.feature_importances_\n",
    "        feature_importances = list(zip(features, importances))\n",
    "\n",
    "        feature_importances_df = pd.DataFrame(feature_importances, columns = [\"features\", \"importance\"]).sort_values(by = \"importance\", ascending = False)\n",
    "\n",
    "        return train_scores, val_scores, feature_importances_df\n",
    "    \n",
    "    except:\n",
    "        return train_scores, val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_path = \"data/6_variables/0_final_variables.csv\"\n",
    "variable_names = md.var_data(2, var_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "-- Model 1 --\n",
      "-------------------------\n",
      "Set structure:\n",
      "Train structure: {1.0: 0.169847087071907, 2.0: 0.8295210413244029, 9.0: 0.0006318716036901302}\n",
      "Validation structure: {1.0: 0.16978271854471955, 2.0: 0.829711975745326, 9.0: 0.0005053057099545225}\n",
      "-------------------------\n",
      ">train score: 0.8295210413244029\n",
      ">test score: 0.829711975745326\n",
      "###########################################################################\n",
      "\n",
      "-- Model 2 --\n",
      "-------------------------\n",
      "Set structure:\n",
      "Train structure: {1.0: 0.169847087071907, 2.0: 0.8296474156451409, 9.0: 0.0005054972829521042}\n",
      "Validation structure: {1.0: 0.16978271854471955, 2.0: 0.8292066700353714, 9.0: 0.001010611419909045}\n",
      "-------------------------\n",
      ">train score: 0.8296474156451409\n",
      ">test score: 0.8292066700353714\n",
      "###########################################################################\n",
      "\n",
      "-- Model 3 --\n",
      "-------------------------\n",
      "Set structure:\n",
      "Train structure: {1.0: 0.16982562547384383, 2.0: 0.8295425827647207, 9.0: 0.0006317917614354309}\n",
      "Validation structure: {1.0: 0.1698685540950455, 2.0: 0.8296258847320526, 9.0: 0.0005055611729019212}\n",
      "-------------------------\n",
      ">train score: 0.8295425827647207\n",
      ">test score: 0.8296258847320526\n",
      "###########################################################################\n",
      "\n",
      "-- Model 4 --\n",
      "-------------------------\n",
      "Set structure:\n",
      "Train structure: {1.0: 0.16982562547384383, 2.0: 0.8295425827647207, 9.0: 0.0006317917614354309}\n",
      "Validation structure: {1.0: 0.1698685540950455, 2.0: 0.8296258847320526, 9.0: 0.0005055611729019212}\n",
      "-------------------------\n",
      ">train score: 0.8295425827647207\n",
      ">test score: 0.8296258847320526\n",
      "###########################################################################\n",
      "\n",
      "-- Model 5 --\n",
      "-------------------------\n",
      "Set structure:\n",
      "Train structure: {1.0: 0.16982562547384383, 2.0: 0.8295425827647207, 9.0: 0.0006317917614354309}\n",
      "Validation structure: {1.0: 0.1698685540950455, 2.0: 0.8296258847320526, 9.0: 0.0005055611729019212}\n",
      "-------------------------\n",
      ">train score: 0.8295425827647207\n",
      ">test score: 0.8296258847320526\n",
      "###########################################################################\n",
      "---> not ok <----\n"
     ]
    }
   ],
   "source": [
    "#model = RandomForestClassifier(n_jobs = -1, random_state = 42)\n",
    "model = LogisticRegression(n_jobs = -1, random_state = 42, max_iter = 300)\n",
    "\n",
    "train_scores, val_scores = ml_trainer(X_train, y_train, kfold, model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            features  importance\n",
       "4    Systolic:  Blood pressure (first reading) mm Hg    0.199815\n",
       "6                                  Total Percent Fat    0.198270\n",
       "5                                        Weight (kg)    0.192193\n",
       "3   Diastolic:  Blood pressure (first reading) mm Hg    0.132394\n",
       "1                         Gender of the participant.    0.132116\n",
       "2  Age in years of the participant at the time of...    0.130182\n",
       "0  The following questions are about different me...    0.015030"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>Systolic:  Blood pressure (first reading) mm Hg</td>\n      <td>0.199815</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Total Percent Fat</td>\n      <td>0.198270</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Weight (kg)</td>\n      <td>0.192193</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Diastolic:  Blood pressure (first reading) mm Hg</td>\n      <td>0.132394</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gender of the participant.</td>\n      <td>0.132116</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Age in years of the participant at the time of...</td>\n      <td>0.130182</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>The following questions are about different me...</td>\n      <td>0.015030</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "source": [
    "print(diet_dfs[\"dr1tot_h\"].shape)\n",
    "print(diet_dfs[\"dr1tot_i\"].shape)\n",
    "print(diet_dfs[\"dr1tot_j\"].shape)\n",
    "\n",
    "files[\"dr1tot\"].shape"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def read_all_data(up_levels, folders):\n",
    "    #dem_folder, diet_folder, exam_folder, lab_folder, quest_folder = folders\n",
    "\n",
    "    dfs_list = []\n",
    "    \n",
    "\n",
    "    for folder in folders:\n",
    "        dfs_list.append(md.read_data(up_levels, folder))\n",
    "\n",
    "    #demo_dfs = md.read_data(up_levels, dem_folder)\n",
    "    #diet_dfs = md.read_data(up_levels, diet_folder)\n",
    "    #exam_dfs = md.read_data(up_levels, exam_folder)\n",
    "    #lab_dfs = md.read_data(up_levels, lab_folder)\n",
    "    #quest_dfs = md.read_data(up_levels, quest_folder)\n",
    "\n",
    "    #return demo_dfs, diet_dfs, exam_dfs, lab_dfs, quest_dfs\n",
    "    return dfs_list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "demo_dfs, diet_dfs, exam_dfs, lab_dfs, quest_dfs = read_all_data(2,[\"1_demographics\", \"2_dietary\", \"3_examination\", \"4_laboratory\", \"5_questionnaire\"])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "data = read_all_data(2, [\"1_demographics\", \"2_dietary\"])\n",
    "data[\"diet_dfs\"][\"dr1tot_j\"]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def var_data(up_levels, filepath):\n",
    "\n",
    "    path = dirname(os.getcwd())\n",
    "    for i in range(up_levels): path = dirname(path)\n",
    "\n",
    "    fullpath = path + sep + filepath\n",
    "    data = pd.read_csv(fullpath, index_col = 0)\n",
    "\n",
    "    return data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}