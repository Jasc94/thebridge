{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candlelight_urls = ['https://feverup.com/m/96831', 'https://feverup.com/m/97975', 'https://feverup.com/m/97053', 'https://feverup.com/m/100404', 'https://feverup.com/m/100896', 'https://feverup.com/m/95040', 'https://feverup.com/m/100405', 'https://feverup.com/m/98160', 'https://feverup.com/m/97280', 'https://feverup.com/m/84521']\n",
    "\n",
    "emojis = emoji.UNICODE_EMOJI[\"en\"]\n",
    "section_emojis = [\"\", \"\", \"\", \"\", \"\", \"锔\", \"\", \"\", \"\", \"\"]\n",
    "info_names = [\"Informaci贸n\", \"Informaci贸n General\", \"Informaci贸n:\", \"Informaci贸n general\"]\n",
    "index = 0\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for url_ in candlelight_urls:\n",
    "    df_ = processor(url_, emojis, section_emojis, info_names, index)\n",
    "\n",
    "    if type(df_) != str:\n",
    "        dfs.append(df_)\n",
    "        print(index, url_)\n",
    "    else:\n",
    "        print(index, url_, df_)\n",
    "\n",
    "    index += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Latest version"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function 1\n",
    "def get_data(url):\n",
    "    # Link to the event\n",
    "    r = requests.get(url)\n",
    "    # Pulling the data from the link\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    # Take event title and description\n",
    "    # In the event description is where \"accesibility\" info is located\n",
    "    event_title = soup.find(class_ = \"plan-hero__title\").text\n",
    "    event_descr = soup.find(class_ = \"plan-description mb-32\")\n",
    "    # Transform the event description into string for later processing\n",
    "    event_descr = str(event_descr)\n",
    "    # Get price info\n",
    "    event_price = soup.find(class_ = \"sidebarBuyingText sidebarWrapper__btn\").text\n",
    "    event_price = event_price.split(\"\\xa0\")[0]\n",
    "\n",
    "    return event_title, event_descr, event_price\n",
    "\n",
    "# Function 2\n",
    "def separate_sections(event_descr):\n",
    "    # 1) Split the data using the \"<strong>\" tag -> This way we separate the sections\n",
    "    # 2) Split the data using the \"</strong>\" tag -> This way we separate titles from descriptions\n",
    "    event_descr_items = [elem.split(\"</strong>\") for elem in event_descr.split(\"<strong>\")]\n",
    "\n",
    "    return event_descr_items\n",
    "\n",
    "# Function 3\n",
    "def separate_title_descr(event_descr_items):\n",
    "    # Dict to save the info title-descriptions that we have in a list of lists\n",
    "    new_dict = {}\n",
    "    # This is to assign a numerical value as key to those descriptions without section title\n",
    "    no_title_count = 1\n",
    "\n",
    "    # Iterate over the list of lists, and for every list...\n",
    "    for elem in event_descr_items:\n",
    "        # If there's more than one element (that means, we have description and title)...\n",
    "        if len(elem) > 1:\n",
    "            # Then the first element will be the key and the second one will be the value in our new dict\n",
    "            new_dict[elem[0]] = elem[1]\n",
    "        # If we don't have two values (we are missing the title)...\n",
    "        else:\n",
    "            # Then, the key will be the numerical value we defined and the value will be the only value of the list, which should be the description\n",
    "            new_dict[no_title_count] = elem[0]\n",
    "            no_title_count += 1\n",
    "\n",
    "    return new_dict\n",
    "\n",
    "# Function 4\n",
    "def remove_html(new_dict):\n",
    "    # This is to remove all the html tags from the text\n",
    "    for key, value in new_dict.items():\n",
    "        new_dict[key] = re.sub(r\"\\<.*?\\>\", \"\", value)\n",
    "\n",
    "    return new_dict\n",
    "\n",
    "# Function 5\n",
    "def separate_info_sections(new_dict):\n",
    "    #Now I have to do all the processing with the iterators to pull the information from \"Informaci贸n general\".\n",
    "\n",
    "    # We'll need an iterator\n",
    "    # iterator = new_dict[\"Informaci贸n General\"]\n",
    "    info_names = [\"Informaci贸n\", \"Informaci贸n General\"]\n",
    "    for name in info_names:\n",
    "        try:\n",
    "            iterator = new_dict[name]\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # We'll need an empty list too, save the subsections\n",
    "    general_info_sections = []\n",
    "\n",
    "    # Lastly, we'll make use of an icons' list to check whether our text has any emoji in it or not\n",
    "    # icons_list = emoji.UNICODE_EMOJI[\"es\"].keys()\n",
    "    icons_list = [\"\", \"\", \"\", \"\", \"\", \"锔\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "    # Iterate over all the icons in the list\n",
    "    for icon in icons_list:\n",
    "        # If the icon is in the iterator (string)...\n",
    "        if icon in iterator:\n",
    "            # Split iterator using the icon and save it as the new iterator\n",
    "            # Input: string\n",
    "            #Output: list with 2 items\n",
    "            iterator = iterator.split(icon)\n",
    "            # Save the first item in our new list\n",
    "            general_info_sections.append(iterator[0])\n",
    "\n",
    "            #Check if there is still any icon left in the second element of the iterator\n",
    "            if any(icon in iterator[1] for icon in icons_list):\n",
    "                # If so, save it as the new iterator (a string again)\n",
    "                iterator = iterator[1]\n",
    "\n",
    "            # Else, save it in our new list.\n",
    "            # As we don't have more icons in the second element, that means, we don't need to split the iterator anymore, since we already reach the last piece of info we needed\n",
    "            else:\n",
    "                general_info_sections.append(iterator[1])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return general_info_sections\n",
    "\n",
    "# Function 6\n",
    "def separate_info_sections_title_descr(general_info_sections):\n",
    "    general_info_sections_2 = []\n",
    "\n",
    "    for elem in general_info_sections:\n",
    "        if len(elem) > 1:\n",
    "            general_info_sections_2.append(elem.split(\":\", 1))\n",
    "\n",
    "    return general_info_sections_2\n",
    "\n",
    "# Function 7\n",
    "def transform_info_sections(general_info_sections_2):\n",
    "    sections = {}\n",
    "    extra = 1\n",
    "\n",
    "    for list_ in general_info_sections_2:\n",
    "        if len(list_) > 1:\n",
    "            sections[list_[0]] = list_[1]\n",
    "        else:\n",
    "            sections[extra] = list_[0]\n",
    "            extra += 1\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Function 8\n",
    "def add_rest(sections, event_title, event_price):\n",
    "    rest = {\"Event_title\": event_title, \"Event_price\": event_price}\n",
    "    final_dict = {**sections, **rest}\n",
    "    return final_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def processor(url):\n",
    "    # Step 1\n",
    "    try:\n",
    "        event_title, event_descr, event_price = get_data(url)\n",
    "    except:\n",
    "        return \"Error in step 1\"\n",
    "    # Step 2\n",
    "    try:\n",
    "        event_descr_items = separate_sections(event_descr)\n",
    "    except:\n",
    "        return \"Error in step 2\"\n",
    "    # Step 3\n",
    "    try:\n",
    "        new_dict = separate_title_descr(event_descr_items)\n",
    "    except:\n",
    "        return \"Error in step 3\"\n",
    "    # Step 4\n",
    "    try:\n",
    "        new_dict = remove_html(new_dict)\n",
    "    except:\n",
    "        return \"Error in step 4\"\n",
    "    # Step 5\n",
    "    try:\n",
    "        general_info_sections = separate_info_sections(new_dict)\n",
    "    except:\n",
    "        return \"Error in step 5\"\n",
    "    # Step 6\n",
    "    try:\n",
    "        general_info_sections_2 = separate_info_sections_title_descr(general_info_sections)\n",
    "    except:\n",
    "        return \"Error in step 6\"\n",
    "    # Step 7\n",
    "    try:\n",
    "        sections = transform_info_sections(general_info_sections_2)\n",
    "    except:\n",
    "        return \"Error in step 7\"\n",
    "    # Step 8\n",
    "    try:\n",
    "        final_dict = add_rest(sections, event_title, event_price)\n",
    "    except:\n",
    "        return \"Error in step 8\"\n",
    "\n",
    "    # Final step\n",
    "    # Creating dfs\n",
    "    general_df = pd.DataFrame(new_dict, index = [0])\n",
    "    sections_df = pd.DataFrame(final_dict, index = [0])\n",
    "\n",
    "    # Joining dfs\n",
    "    full_df = pd.merge(general_df, sections_df, how = \"outer\", left_index = True, right_index = True)\n",
    "\n",
    "    return full_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Art\n",
    "#url = \"https://feverup.com/m/100122\"\n",
    "#url = \"https://feverup.com/m/100982\"\n",
    "#url = \"https://feverup.com/m/96379\"\n",
    "url = \"https://feverup.com/m/100122\"\n",
    "\n",
    "#processor(url)\n",
    "#Step 1: Get data\n",
    "event_title, event_descr, event_price = get_data(url)\n",
    "# Step 2: Split event_descr into sections\n",
    "event_descr_items = separate_sections(event_descr)\n",
    "#Step 3: Split sections into titles and descriptions\n",
    "title_descr = separate_title_descr(event_descr_items)\n",
    "# Step 4: Some cleaning\n",
    "# 4.1 Remove html\n",
    "title_descr_without_html = remove_html(title_descr)\n",
    "\n",
    "# 4.2 Remove emojis\n",
    "emojis = emoji.UNICODE_EMOJI[\"en\"]\n",
    "info_names = [\"Informaci贸n\", \"Informaci贸n General\"]\n",
    "cleaned_dict = remove_all_emojis(title_descr_without_html, emojis, info_names)\n",
    "\n",
    "# Step 5: Get data from \"info\" section\n",
    "icons_list = [\"\", \"\", \"\", \"\", \"\", \"锔\", \"\", \"\", \"\", \"\"]\n",
    "general_info_sections = separate_info_sections(title_descr_without_html, info_names, icons_list)\n",
    "\n",
    "#5.1 Split the data from \"info\" section into title and descr\n",
    "new_general_info_sections = separate_info_sections_title_descr(general_info_sections)\n",
    "\n",
    "#Step 6: Make the \"info\" subsections a list\n",
    "info_sections = transform_info_sections(new_general_info_sections)\n",
    "\n",
    "#Step 7: Join all the data together into a dict\n",
    "final_dict = add_rest(info_sections, event_title, event_price)\n",
    "\n",
    "#Step 8: Create joined dataframe\n",
    "df = create_df(cleaned_dict, final_dict, 0)\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sacar URLs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "url_madrid = \"https://feverup.com/\"\n",
    "\n",
    "r = requests.get('https://feverup.com/madrid')\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "eventos_g = soup.findAll(class_ = \"fv-carousel__feed\")\n",
    "eventos_g = soup.findAll(class_ = \"fv-carousel__item\")\n",
    "\n",
    "\n",
    "lista_URLs = []\n",
    "for evento in eventos_g:\n",
    "    URL_evento = evento.find('a')['href']\n",
    "    lista_URLs.append(URL_evento)\n",
    "\n",
    "lista_completa = []\n",
    "for url in lista_URLs:\n",
    "    url_completa = url_madrid + url\n",
    "    lista_completa.append(url_completa)\n",
    "#lista_completa"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch = lista_completa[:20]\n",
    "\n",
    "dfs = []\n",
    "count = 1\n",
    "error_count = 0\n",
    "error_log = []\n",
    "\n",
    "for url_ in batch:\n",
    "    new_df = processor(url_)\n",
    "\n",
    "    if type(new_df) != str:\n",
    "        print(count, \":\", url_)\n",
    "        dfs.append(new_df)\n",
    "    else:\n",
    "        print(count, \":\", url_ + \" --> error\")\n",
    "        error_log.append(f\"Iteraci贸n: {count} - link: {url_} - step with error: {new_df}\")\n",
    "        error_count += 1\n",
    "\n",
    "    count += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "error_count / count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.concat(dfs)\n",
    "print(df.shape)\n",
    "df.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#re.split(\"Informaci贸n\", text)\n",
    "#items = re.findall(r\"(?<=strong).+\", text_with_html)\n",
    "items = text_with_html.split(\"strong\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gen_info = re.sub(r\"\\<.*?\\>\", \"\", items[6])\n",
    "gen_info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "info_items_2 = []\n",
    "for item in info_items: info_items_2.append(re.sub(r\"\\<.*?\\>\", \"\", item))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gen_info = info_items_2[6].split(\"\")\n",
    "date = gen_info[0]\n",
    "\n",
    "gen_info_2 = gen_info[1].split(\"\")\n",
    "timetable = gen_info_2[0]\n",
    "\n",
    "gen_info_3 = gen_info_2[1].split(\"\")\n",
    "time = gen_info_3[0]\n",
    "time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iterator = info_items[6]\n",
    "\n",
    "icons = [\"\", \"\", \"\", \"\", \"\", \"锔\", \"\", \"\", \"\", \"\"]\n",
    "general_info = []\n",
    "count = 0\n",
    "\n",
    "for icon in icons:\n",
    "    #print(\"icon:\", icon)\n",
    "    #print(\"iterator:\\n\", iterator)\n",
    "    #print(\"-\" * 20)\n",
    "    if icon in iterator:\n",
    "        iterator = iterator.split(icon)\n",
    "        #print(\"iterator[0]:\\n\", iterator[0])\n",
    "        #print(\"-\" * 20)\n",
    "        general_info.append(iterator[0])\n",
    "        #print(\"iterator[1]:\\n\", iterator[1])\n",
    "        if any(icon in iterator[1] for icon in icons):\n",
    "            iterator = iterator[1]\n",
    "        else:\n",
    "            general_info.append(iterator[1])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    count += 1\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "general_info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function 1\n",
    "def get_data(url):\n",
    "    # Link to the event\n",
    "    r = requests.get(url)\n",
    "    # Pulling the data from the link\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    # Take event title and description\n",
    "    # In the event description is where \"accesibility\" info is located\n",
    "    event_title = soup.find(class_ = \"plan-hero__title\").text\n",
    "    event_descr = soup.find(class_ = \"plan-description mb-32\")\n",
    "    # Transform the event description into string for later processing\n",
    "    event_descr = str(event_descr)\n",
    "    # Get price info\n",
    "    event_price = soup.find(class_ = \"sidebarBuyingText sidebarWrapper__btn\").text\n",
    "    event_price = event_price.split(\"\\xa0\")[0]\n",
    "\n",
    "    return event_title, event_descr, event_price\n",
    "\n",
    "# Function 2\n",
    "def separate_sections(event_descr):\n",
    "    # 1) Split the data using the \"<strong>\" tag -> This way we separate the sections\n",
    "    # 2) Split the data using the \"</strong>\" tag -> This way we separate titles from descriptions\n",
    "    event_descr_items = [elem.split(\"</strong>\") for elem in event_descr.split(\"<strong>\")]\n",
    "\n",
    "    return event_descr_items\n",
    "\n",
    "# Function 3\n",
    "def separate_title_descr(event_descr_items):\n",
    "    # Dict to save the info title-descriptions that we have in a list of lists\n",
    "    title_descr = {}\n",
    "    # This is to assign a numerical value as key to those descriptions without section title\n",
    "    no_title_count = 1\n",
    "\n",
    "    # Iterate over the list of lists, and for every list...\n",
    "    for elem in event_descr_items:\n",
    "        # If there's more than one element (that means, we have description and title)...\n",
    "        if len(elem) > 1:\n",
    "            # Then the first element will be the key and the second one will be the value in our new dict\n",
    "            new_dict[elem[0]] = elem[1]\n",
    "        # If we don't have two values (we are missing the title)...\n",
    "        else:\n",
    "            # Then, the key will be the numerical value we defined and the value will be the only value of the list, which should be the description\n",
    "            new_dict[no_title_count] = elem[0]\n",
    "            no_title_count += 1\n",
    "\n",
    "    return title_descr\n",
    "\n",
    "# Function 4\n",
    "def remove_html(title_descr):\n",
    "    # This is to remove all the html tags from the text\n",
    "    for key, value in title_descr.items():\n",
    "        title_descr[key] = re.sub(r\"\\<.*?\\>\", \"\", value)\n",
    "\n",
    "    return title_descr_without_html\n",
    "\n",
    "# Function 5\n",
    "def separate_info_sections(title_descr_without_html):\n",
    "    #Now I have to do all the processing with the iterators to pull the information from \"Informaci贸n general\".\n",
    "\n",
    "    # We'll need an iterator\n",
    "    # iterator = new_dict[\"Informaci贸n General\"]\n",
    "    info_names = [\"Informaci贸n\", \"Informaci贸n General\"]\n",
    "    for name in info_names:\n",
    "        try:\n",
    "            iterator = title_descr_without_html[name]\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # We'll need an empty list too, save the subsections\n",
    "    general_info_sections = []\n",
    "\n",
    "    # Lastly, we'll make use of an icons' list to check whether our text has any emoji in it or not\n",
    "    # icons_list = emoji.UNICODE_EMOJI[\"es\"].keys()\n",
    "    icons_list = [\"\", \"\", \"\", \"\", \"\", \"锔\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "    # Iterate over all the icons in the list\n",
    "    for icon in icons_list:\n",
    "        # If the icon is in the iterator (string)...\n",
    "        if icon in iterator:\n",
    "            # Split iterator using the icon and save it as the new iterator\n",
    "            # Input: string\n",
    "            #Output: list with 2 items\n",
    "            iterator = iterator.split(icon)\n",
    "            # Save the first item in our new list\n",
    "            general_info_sections.append(iterator[0])\n",
    "\n",
    "            #Check if there is still any icon left in the second element of the iterator\n",
    "            if any(icon in iterator[1] for icon in icons_list):\n",
    "                # If so, save it as the new iterator (a string again)\n",
    "                iterator = iterator[1]\n",
    "\n",
    "            # Else, save it in our new list.\n",
    "            # As we don't have more icons in the second element, that means, we don't need to split the iterator anymore, since we already reach the last piece of info we needed\n",
    "            else:\n",
    "                general_info_sections.append(iterator[1])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return general_info_sections\n",
    "\n",
    "# Function 6\n",
    "def separate_info_sections_title_descr(general_info_sections):\n",
    "    \n",
    "    general_info_sections_2 = []\n",
    "\n",
    "    for elem in general_info_sections:\n",
    "        if len(elem) > 1:\n",
    "            general_info_sections_2.append(elem.split(\":\", 1))\n",
    "\n",
    "    return general_info_sections_2\n",
    "\n",
    "# Function 7\n",
    "def transform_info_sections(general_info_sections_2):\n",
    "    sections = {}\n",
    "    extra = 1\n",
    "\n",
    "    for list_ in general_info_sections_2:\n",
    "        if len(list_) > 1:\n",
    "            sections[list_[0]] = list_[1]\n",
    "        else:\n",
    "            sections[extra] = list_[0]\n",
    "            extra += 1\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Function 8\n",
    "def add_rest(sections, event_title, event_price):\n",
    "    rest = {\"Event_title\": event_title, \"Event_price\": event_price}\n",
    "    final_dict = {**sections, **rest}\n",
    "    return final_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def processor(url):\n",
    "    # Step 1\n",
    "    try:\n",
    "        event_title, event_descr, event_price = get_data(url)\n",
    "    except:\n",
    "        return \"Error in step 1\"\n",
    "    # Step 2\n",
    "    try:\n",
    "        event_descr_items = separate_sections(event_descr)\n",
    "    except:\n",
    "        return \"Error in step 2\"\n",
    "    # Step 3\n",
    "    try:\n",
    "        new_dict = separate_title_descr(event_descr_items)\n",
    "    except:\n",
    "        return \"Error in step 3\"\n",
    "    # Step 4\n",
    "    try:\n",
    "        new_dict = remove_html(new_dict)\n",
    "    except:\n",
    "        return \"Error in step 4\"\n",
    "    # Step 5\n",
    "    try:\n",
    "        general_info_sections = separate_info_sections(new_dict)\n",
    "    except:\n",
    "        return \"Error in step 5\"\n",
    "    # Step 6\n",
    "    try:\n",
    "        general_info_sections_2 = separate_info_sections_title_descr(general_info_sections)\n",
    "    except:\n",
    "        return \"Error in step 6\"\n",
    "    # Step 7\n",
    "    try:\n",
    "        sections = transform_info_sections(general_info_sections_2)\n",
    "    except:\n",
    "        return \"Error in step 7\"\n",
    "    # Step 8\n",
    "    try:\n",
    "        final_dict = add_rest(sections, event_title, event_price)\n",
    "    except:\n",
    "        return \"Error in step 8\"\n",
    "\n",
    "    # Final step\n",
    "    # Creating dfs\n",
    "    general_df = pd.DataFrame(new_dict, index = [0])\n",
    "    sections_df = pd.DataFrame(final_dict, index = [0])\n",
    "\n",
    "    # Joining dfs\n",
    "    full_df = pd.merge(general_df, sections_df, how = \"outer\", left_index = True, right_index = True)\n",
    "\n",
    "    return full_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "art_urls = ['https://feverup.com/m/97759', 'https://feverup.com/m/87806', 'https://feverup.com/m/92796',\n",
    "             'https://feverup.com/m/92640', 'https://feverup.com/m/96367', 'https://feverup.com/m/94382',\n",
    "             'https://feverup.com/m/92141']\n",
    "\n",
    "emojis = emoji.UNICODE_EMOJI[\"en\"]\n",
    "section_emojis = [\"\", \"\", \"\", \"\", \"\", \"锔\", \"\", \"\", \"\", \"\"]\n",
    "info_names = [\"Informaci贸n\", \"Informaci贸n General\", \"Informaci贸n:\", \"Informaci贸n general\"]\n",
    "index = 0\n",
    "\n",
    "df_2 = multiple_processor(art_urls, emojis, section_emojis, info_names, index)\n",
    "print(df_2.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "outdoor_urls = ['https://feverup.com/m/100044', 'https://feverup.com/m/100978', \n",
    "                'https://feverup.com/m/100603', 'https://feverup.com/m/100497',\n",
    "                'https://feverup.com/m/99411', 'https://feverup.com/m/100896',\n",
    "                'https://feverup.com/m/74440', 'https://feverup.com/m/98057',\n",
    "                'https://feverup.com/m/93402', 'https://feverup.com/m/80284']\n",
    "\n",
    "emojis = emoji.UNICODE_EMOJI[\"en\"]\n",
    "section_emojis = [\"\", \"\", \"\", \"\", \"\", \"锔\", \"\", \"\", \"\", \"\"]\n",
    "info_names = [\"Informaci贸n\", \"Informaci贸n General\", \"Informaci贸n:\", \"Informaci贸n general\"]\n",
    "index = 0\n",
    "\n",
    "df_3 = multiple_processor(outdoor_urls, emojis, section_emojis, info_names, index)\n",
    "print(df_3.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1)Arte\n",
    "# 2)Atracciones\n",
    "# 3)Brunch\n",
    "# 4)Dinner\n",
    "# 5)Terrazas\n",
    "# 6)Deportes\n",
    "# 7)Conciertos\n",
    "# 8)Actividades\n",
    "# 9)Belleza\n",
    "# 10)Afterwork\n",
    "# 11)Clubs\n",
    "# 12)Aire_libre\n",
    "# 13)Candlelights\n",
    "# 14)Tributos\n",
    "# 15)Gastronom铆a\n",
    "# 16)Cine\n",
    "# 17)Teatro"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}